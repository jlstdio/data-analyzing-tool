{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-15T05:48:19.529286Z",
     "start_time": "2025-01-15T05:48:19.504412Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/eis\n",
      "/disk2/jl/jl/data_analyzing_tool\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eis/anaconda3/envs/jl_maa/lib/python3.10/site-packages/IPython/core/magics/osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n",
      "/home/eis/anaconda3/envs/jl_maa/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd ~\n",
    "%cd ../../disk2/jl/jl/data_analyzing_tool\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All CIFAR-10 files are present.\n",
      "Computed mean: [125.30691805 122.95039414 113.86538318]\n",
      "Computed std: [62.99321928 62.08870764 66.70489964]\n",
      "All CIFAR-10 files are present.\n",
      "CIFAR-10 loaded.\n",
      "All CIFAR-10 files are present.\n",
      "CIFAR-10 loaded.\n",
      "All CIFAR-10 files are present.\n",
      "CIFAR-10 loaded.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload \n",
    "from data_prepare.data_prepare_manager import select_dataset\n",
    "\n",
    "dataset_name = \"cifar-10\"\n",
    "dataset_A, classes_A = select_dataset(dataset_name)\n",
    "\n",
    "dataset_name = \"cifar-10_jitter\"\n",
    "dataset_B, classes_B = select_dataset(dataset_name)\n",
    "\n",
    "dataset_name = \"cifar-10_rotate\"\n",
    "dataset_C, classes_C = select_dataset(dataset_name)\n",
    "\n",
    "dataset_name = \"cifar-10_noise\"\n",
    "dataset_D, classes_D = select_dataset(dataset_name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-15T05:49:26.146848Z",
     "start_time": "2025-01-15T05:48:26.718884Z"
    }
   },
   "id": "b36e5efe98e5c868"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from model_structure.cnn_model_2d import CNN2D\n",
    "\n",
    "classes = classes_A = classes_B = classes_C = classes_D\n",
    "num_classes = len(classes)\n",
    "print(num_classes)\n",
    "# from model_structure.testNN_wo_softmax import testNN_wo_Softmax\n",
    "# model = testNN_wo_Softmax(outputClasses=num_classes)\n",
    "\n",
    "from model_structure.testModel_wo_softmax_5_layer import testNN_wo_Softmax_5_layer\n",
    "model = testNN_wo_Softmax_5_layer(outputClasses=num_classes)\n",
    "# model = CNN2D(num_classes=9)\n",
    "# pth_path = './trained_pth/rootModel-da 1 - 0.pth'\n",
    "# pth_path = './trained_pth/best_model_pretrain-1734930660.pth'\n",
    "\n",
    "# CLustered Pick\n",
    "'''\n",
    "pth_path_list = ['./trained_pth/rootModel-fed_avg - MD Mixed | RP.pth',\n",
    "                 './trained_pth/rootModel-fed_prox - MD Mixed | RP | Mu 0.001.pth',\n",
    "                 './trained_pth/rootModel-fisher_client - MD Mixed | RP | Mu 2.0.pth',\n",
    "                 './trained_pth/rootModel-fisher_server - MD Mixed | RP | Mu 1.0.pth',\n",
    "                 './trained_pth/rootModel-DA| weighted fed_avg_param_diff - MD Mixed | RP.pth']\n",
    "'''\n",
    "\n",
    "'''\n",
    "pth_path_list = ['./trained_pth/rootModel-fed_avg - MD Mixed | RP.pth',\n",
    "                 './trained_pth/rootModel-fed_prox - MD Mixed | RP | Mu 0.001.pth',\n",
    "                 './trained_pth/rootModel-DA| weighted fed_avg_param_diff - MD Mixed | RP.pth']\n",
    "'''\n",
    "\n",
    "pth_path_list = ['./trained_pth/rootModel-fed_avg_more_filters - MD Mixed | RP.pth']\n",
    "# pth_path_list = ['./trained_pth/rootModel-fed_avg_5_layer - MD Mixed | RP.pth']\n",
    "\n",
    "\n",
    "# './trained_pth/rootModel-fed_avg - SD_original | RP.pth',"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-15T06:02:39.787865Z",
     "start_time": "2025-01-15T06:02:39.723359Z"
    }
   },
   "id": "8024c622ce8cd181"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "features = []  # 전역 리스트 (hook_fn에서 사용)\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    \"\"\"\n",
    "    지정된 레이어(예: conv3)의 '입력' 혹은 '출력'을 캡처하는 Hook 함수.\n",
    "    여기서는 input[0]을 사용하므로, conv3 레이어의 입력 텐서를 수집합니다.\n",
    "    \n",
    "    - 어떤 레이어에 걸고 싶은지는 모델 구조에 따라 변경하세요.\n",
    "    - 혹은 output을 보려면 output을 사용하세요.\n",
    "    \"\"\"\n",
    "    # Hook에 들어오는 input[0]은 (batch_size, channel, height, width) 형태일 가능성이 큽니다.\n",
    "    # 필요하다면 .detach() 처리 후 numpy()로 변환하세요.\n",
    "    # GPU->CPU 변환 위해 .cpu() 사용, 미분 그래프에서 분리 위해 .detach() 사용\n",
    "    batch_features = input[0].detach().cpu().numpy()\n",
    "    features.append(batch_features)\n",
    "    \n",
    "def extract_per_sample_features(model, inference_instance, layer_name):\n",
    "    \"\"\"\n",
    "    주어진 모델과 inference_instance(= 데이터셋 + forward)를 이용해\n",
    "    전체 데이터셋 각 샘플별 feature를 추출하여 반환.\n",
    "    \"\"\"\n",
    "    global features\n",
    "    features = []  # 매 호출 시 초기화\n",
    "\n",
    "    # Hook 등록 (conv3 레이어)\n",
    "    # hook = model.conv3.register_forward_hook(hook_fn)\n",
    "    # hook = model.conv1.register_forward_hook(hook_fn)\n",
    "    \n",
    "    layer = getattr(model, layer_name, None)\n",
    "    hook = layer.register_forward_hook(hook_fn)\n",
    "\n",
    "    # (아래 코드는 예시입니다. 실제론 inference_instance.examin()이\n",
    "    # 전체 샘플에 대해 forward, loss/acc 계산을 수행한다고 가정)\n",
    "    avg_loss, acc, all_targets, all_outputs = inference_instance.examin()\n",
    "\n",
    "    # Hook 해제\n",
    "    hook.remove()\n",
    "\n",
    "    # features: 각 배치마다 쌓인 numpy 배열들의 리스트\n",
    "    # -> shape (num_samples, C, H, W) 로 합치기\n",
    "    extracted_features = np.concatenate(features, axis=0)\n",
    "\n",
    "    # 필요에 따라 (num_samples, C*H*W) 형태로 리쉐이프\n",
    "    B, C, H, W = extracted_features.shape\n",
    "    extracted_features = extracted_features.reshape(B, -1)\n",
    "\n",
    "    return extracted_features, all_targets  # (num_samples, feature_dim), (num_samples, )\n",
    "\n",
    "##########################################\n",
    "# 2) 샘플별 L2 / Cosine 값 전체를 구하는 함수 #\n",
    "##########################################\n",
    "\n",
    "def calculate_per_sample_distances(original_features, compared_features):\n",
    "    \"\"\"\n",
    "    original_features와 compared_features 각각 (num_samples, feature_dim) 형태.\n",
    "    동일 인덱스끼리 L2/Cosine을 구해 (num_samples,) 형태로 반환.\n",
    "    \"\"\"\n",
    "    eps = 1e-8\n",
    "    \n",
    "    # L2 distance\n",
    "    diff = original_features - compared_features  # (num_samples, feature_dim)\n",
    "    l2_all = np.sqrt(np.sum(diff**2, axis=1))    # (num_samples,)\n",
    "    \n",
    "    # L2 distance normalization\n",
    "    x_norm = np.sqrt(np.sum(original_features**2, axis=1))  # (num_samples,)\n",
    "    y_norm = np.sqrt(np.sum(compared_features**2, axis=1))  # (num_samples,)\n",
    "    l2_normalized = l2_all / (x_norm + y_norm + eps)        # (num_samples,)\n",
    "\n",
    "    # Cosine similarity\n",
    "    dot_xy = np.sum(original_features * compared_features, axis=1)\n",
    "    x_norm = np.sqrt(np.sum(original_features**2, axis=1))\n",
    "    y_norm = np.sqrt(np.sum(compared_features**2, axis=1))\n",
    "    cos_all = dot_xy / (x_norm * y_norm + eps)   # (num_samples,)\n",
    "\n",
    "    return l2_normalized, cos_all"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-15T06:03:41.282681Z",
     "start_time": "2025-01-15T06:03:41.256746Z"
    }
   },
   "id": "5946b4222facdd2f"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CKA value: 0.3420460522174835\n"
     ]
    }
   ],
   "source": [
    "from cka_analysis.cka import *\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 50).astype(np.float32)  # Use consistent dtype\n",
    "Y = np.random.rand(100, 50).astype(np.float32)  # Use consistent dtype\n",
    "\n",
    "K1 = linear_kernel(X)\n",
    "K2 = linear_kernel(Y)\n",
    "\n",
    "cka_value = cka(K1, K2)\n",
    "print(f\"CKA value: {cka_value}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-15T06:03:47.712387Z",
     "start_time": "2025-01-15T06:03:47.678417Z"
    }
   },
   "id": "d0d7df01879a41ed"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4de8ce5a91708b96"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "jl_maa",
   "language": "python",
   "display_name": "jl_maa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
